{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2620e6",
   "metadata": {},
   "source": [
    "# CS 5110: Data Privacy Final - Nicholas Kent\n",
    "## Machine Learning Model Using DP-Mini-Batching Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2c38261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import gen_batches\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# modified guassian mechanism to work with a npy array\n",
    "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
    "    noised_data = np.array([np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon, size = vec.shape[1])\n",
    "                            for _ in range(vec.shape[0])])\n",
    "    return vec + noised_data\n",
    "\n",
    "\n",
    "# Download the processed csv files\n",
    "# Define vars to store:\n",
    "# The original dataset for reference\n",
    "athlete = pd.read_csv('https://raw.githubusercontent.com/nichkent/Data-Privacy-Final/main/athlete_events.csv')\n",
    "\n",
    "# The x or sample columns\n",
    "athlete_x = pd.read_csv('https://raw.githubusercontent.com/nichkent/Data-Privacy-Final/main/athlete_events_processed_x.csv')\n",
    "\n",
    "# The y or target columns\n",
    "athlete_y = pd.read_csv('https://raw.githubusercontent.com/nichkent/Data-Privacy-Final/main/athlete_events_processed_y.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eeba81",
   "metadata": {},
   "source": [
    "## Part 1: Prepare athlete_x and athlete_y For the Model\n",
    "\n",
    "In this part, the information stored in the two csv files loaded \n",
    "into `athlete_x` and `athlete_y` are prepared and processed so that they\n",
    "can be loaded into the machine learning model later.\n",
    "\n",
    "- This required removing unnecessary columns that would not be useful in dertermining who is most likely to predict the target set. Also, in  this same stepthe **Name** and **Event** columns were removed here to be used  later when determining the athlete who is most likely to win a gold medal.\n",
    "\n",
    "- After saving the Name and event columns to separate numpy files the only categorical column left was the **Sex** column. This columns information was One-Hot-Encoded using Sklearn's OneHotEncoder.\n",
    "\n",
    "- Since there are NA values present in the data, we were required to remove them in some way. To do this I decided to use Sklearn's SimpleImputer to impute the NA values of the data to the mean of the current column. This implementation resolves an issue that would have occured when attempting to use `Logistic Regression` later in the model.\n",
    "\n",
    "Finally, the manipulated datasets are stored in their respective numpy arrays to be fed to the model during the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25b202ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert all feature names to strings\n",
    "athlete_x.columns = [str(col) for col in athlete_x.columns]\n",
    "\n",
    "# Save the 'Name' and 'Event' columns separately before dropping them\n",
    "# This is so that we can reassign the final values for who got the medal at the end\n",
    "# Save to separate numpy files\n",
    "names_array = athlete_x['Name'].values\n",
    "events_array = athlete_x['Event'].values\n",
    "np.save('names.npy', names_array)\n",
    "np.save('events.npy', events_array)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Drop all irrelevent columns\n",
    "\n",
    "# Drop irrelevant columns including 'Name' and 'Event' now that they have been saved\n",
    "columns_to_drop = ['ID', 'Games', 'Team', 'NOC', 'Year', 'Season', 'City', 'Sport', 'Name', 'Event']\n",
    "\n",
    "# Drop unnecessary columns \n",
    "for col in columns_to_drop:\n",
    "    if col in athlete_x.columns:\n",
    "        athlete_x.drop(col, axis = 1, inplace = True)\n",
    "\n",
    "        \n",
    "# Step 3: One Hot Encode the categorical variables\n",
    "\n",
    "# Handling categorical data using one-hot encoding\n",
    "# Instantiate the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse = False)        \n",
    "        \n",
    "# Handling categorical data using one-hot encoding\n",
    "# 'Sex' is the only remaining categorical column in athlete_x\n",
    "categorical_columns = ['Sex'] \n",
    "categorical_data = encoder.fit_transform(athlete_x[categorical_columns])\n",
    "\n",
    "# Create meaningful column names for the one-hot encoded column\n",
    "columns = encoder.get_feature_names(categorical_columns)  \n",
    "categorical_df = pd.DataFrame(categorical_data, columns = columns)\n",
    "\n",
    "# Reset the index on the original DataFrame to ensure alignment\n",
    "athlete_x.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Drop the original categorical columns in athlete_x\n",
    "athlete_x = athlete_x.drop(categorical_columns, axis = 1)\n",
    "\n",
    "\n",
    "# Step 4: Change the NA values to the mean value of that column in athlete_x\n",
    "\n",
    "numeric_columns = athlete_x.select_dtypes(include=[np.number]).columns\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "# Perform the calculation and convert back to DataFrame to maintain column names\n",
    "athlete_x_numeric = pd.DataFrame(imputer.fit_transform(athlete_x[numeric_columns]), columns = numeric_columns)\n",
    "\n",
    "# Concatenate the numeric DataFrame and the one-hot encoded categorical DataFrame\n",
    "athlete_x = pd.concat([athlete_x_numeric, categorical_df], axis = 1)\n",
    "\n",
    "# Convert the DataFrame to a Numpy array\n",
    "data_array = athlete_x.values\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate and apply a scaler to fix overfitting issues of the model\n",
    "scaler = StandardScaler()\n",
    "data_array_scaled = scaler.fit_transform(data_array)\n",
    "\n",
    "# Save the arrays to a .npy file\n",
    "# Create the sample array\n",
    "np.save('athlete_events_processed_x.npy', data_array_scaled)\n",
    "\n",
    "# Create the target array\n",
    "medals_array = athlete_y.values\n",
    "np.save('athlete_events_processed_y.npy', medals_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1711f82",
   "metadata": {},
   "source": [
    "## Part 2: Load Numpy Arrays For Learning\n",
    "\n",
    "After the data has been successfully preprocessed for the model, it must now be loaded into training and test sets for the model to learn off of. This was divided into training sets of size `119999` and test sets of size `30000` which adds up to the total `149999` size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f0b343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train and test set sizes: 119999 30000\n",
      "Y Train and test set sizes: 119999 30000\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Define the vars for the learning model\n",
    "\n",
    "# Load the sample and target arrays from their respective files\n",
    "X = np.load('athlete_events_processed_x.npy')\n",
    "y = np.load('athlete_events_processed_y.npy')\n",
    "\n",
    "# Split data into training and test sets\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "# Create train and test sets\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "# Print the size of the train and test sets\n",
    "print('X Train and test set sizes:', len(X_train), len(X_test))\n",
    "print('Y Train and test set sizes:', len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e8fb5",
   "metadata": {},
   "source": [
    "## Part 3: Create The Necessary Tools For Learning\n",
    "\n",
    "In order to create the model for learning it was required that class weights be used to remove weight baises from the columns. This technique involves the usage of Sklearn's `compute_class_weight` function that allows us to balance the y_train array.\n",
    "\n",
    "Then the `gaussian mechanism` is run on the X_train dataset. Since this is the information being passed into the learning model we need to add noise. The gaussian mechanism is used here, specifically the vectorized version of the function. This instantiation of the gaussian mechanism has a total privacy cost of 1 under sequential composition with a sensitivity of 1.\n",
    "\n",
    "The `create_model` function is called in part 4. This function uses **Stochastic Gradient Descent** with **Logistic Regression** in order to train the model with the X_train and y_train datasets. The choice to have max_iter = 100 is due to computational limitations. This value can be adjusted based on available resources and required model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ffc945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped y_train type: <class 'numpy.ndarray'>\n",
      "Reshaped y_train shape: (119999,)\n",
      "Class weights computed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Define the learning model with Stochastic\n",
    "\n",
    "# Runs for 3 iterations due to computational limitations, could theoretically run for more\n",
    "# Uses logistic regression\n",
    "\n",
    "# For the class weight later, imported here to not interfer with previous code\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Reshape y_train to be a 1D array\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# Print the reshaped y_train's type and shape\n",
    "print(\"Reshaped y_train type:\", type(y_train))\n",
    "print(\"Reshaped y_train shape:\", y_train.shape)\n",
    "\n",
    "# Now compute class weights to remove weight baises when running\n",
    "try:\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight = 'balanced', \n",
    "        classes = np.unique(y_train), \n",
    "        y = y_train\n",
    "    )\n",
    "    \n",
    "    print(\"Class weights computed successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error computing class weights:\", e)\n",
    "\n",
    "# Ensure that class labels are mapped correctly to their weights\n",
    "class_labels = np.unique(y_train)\n",
    "class_weight_dict = {class_labels[i]: class_weights[i] for i in range(len(class_labels))}\n",
    "\n",
    "\n",
    "# Make sure X_train is a 2D numpy array\n",
    "X_train_array = np.array(X_train)  \n",
    "\n",
    "# Apply differential privacy noise to the training data before passing the information to the model\n",
    "epsilon = 1.0  # Differential privacy parameter using sequential composition\n",
    "delta = 1e-5   # Delta parameter for Gaussian mechanism\n",
    "X_train_noisy = gaussian_mech_vec(X_train, sensitivity = 1, epsilon = epsilon, delta = delta)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # Uses logistical regression\n",
    "    # Iterations currently set to 100, 1000 is likely to work better however but this is due to computational limits\n",
    "    # Uses Stochastic Gradient Descent because we are using mini-batching gradient descent defined below which requires SGD as a basis\n",
    "    model = SGDClassifier(loss='log', max_iter = 10, tol = 1e-3, class_weight = class_weight_dict)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204791c",
   "metadata": {},
   "source": [
    "## Part 4: Using The Mini-Batching Technique\n",
    "\n",
    "The implementation of **Mini-Batching Gradient Descent** uses a batch size of 32. This was found to be the best trade-off of computational efficiency and this model's ability to generalize given the training data.\n",
    "\n",
    "1. The `create_model` function is called to instatiate a **Logistic Regression** model using **Stochastic Gradient Descent**.\n",
    "2. The model is trained over a number of iterations (100). Each iteration the entire dataset is passed through the model in mini-batches.\n",
    "    - At the beginning of each loop the indicies of the training data are shuffled. This shuffling is integral to mini-batch training to ensure that each batch is filled with random data from the dataset. This makes sure that he model does not memorize the dataset with pattern recognition.\n",
    "    - The training data is then divided into mini-batches. For each batch, a subset of the data is selected based on the shuffled indices. This subset includes both the features of `X_batch` and the labels of `y_batch`.\n",
    "    - Each mini-btach is then used to partially fit the model with the `partial_fit` method. This helps with reducing computational intensity of the program.\n",
    "3. The model is then evaluated using Sklearn's `classification_report` function. This prints out a few numbers including precision, recall, f1-score for each class, and the probability of which athlete is most likely to win a gold medal based on the dataset. The athlete array is then recombined here to help identifiy this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ebf7d6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86     28502\n",
      "           1       0.07      0.33      0.12      1498\n",
      "\n",
      "    accuracy                           0.76     30000\n",
      "   macro avg       0.51      0.55      0.49     30000\n",
      "weighted avg       0.91      0.76      0.82     30000\n",
      "\n",
      "Athlete most likely to win gold: Max Liebermann with probability 0.9419\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Run the mini-batching\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the batch size for mini-batch training\n",
    "batch_size = 32  \n",
    "\n",
    "# Create the logistic regression model using SGD with mini-batch learning\n",
    "model = create_model()\n",
    "\n",
    "# Perform mini-batch training with noised data\n",
    "for epoch in range(model.max_iter):\n",
    "    # Create shuffled indicies\n",
    "    shuffled_indices = np.random.permutation(len(X_train_noisy))\n",
    "    \n",
    "    # Run through the noisy dataset\n",
    "    for i in range(0, len(X_train_noisy), batch_size):\n",
    "        # Find the batch indicies\n",
    "        batch_indices = shuffled_indices[i:i + batch_size]\n",
    "        \n",
    "        # Create the batches for X and y respectively using the batch indicies found above\n",
    "        X_batch = X_train_noisy[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        \n",
    "        # Fit the new batches to the model\n",
    "        model.partial_fit(X_batch, y_batch, classes = np.array([0, 1]))\n",
    "\n",
    "        \n",
    "# Step 8: Evaluate the model's performance\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict the probability of each athlete in the test set winning a gold medal\n",
    "probabilities = model.predict_proba(X_test)[:, 1]\n",
    "max_prob_index = np.argmax(probabilities)\n",
    "prob = probabilities[max_prob_index]\n",
    "prob_str = f\"{prob:.4f}\" if prob < 1 else \"1.0000 (very high confidence)\"\n",
    "print(f\"Athlete most likely to win gold: {names_array[training_size:][max_prob_index]} with probability {prob_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8848b3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The model, given it's current parameters, is able to predict an athlete that is most likely to win gold based on the given columns with about 50-60% accuracy. A limitation of this implementation is that it is very computationally intensive. To get a result of 80% or higher, which would be ideal in, would require a lot of computational power to increase the number of iterations. The implemenation of the gaussian mechanism just before training the model ensures differential privacy of the output by adding noise to the entire dataset thus ensuring that the model could not memorize the dataset given to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728de88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
